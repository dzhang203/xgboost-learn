{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Notebook: Basic Walkthrough of XGBoost\n",
    "@dzhang203 // init 2019-05-21, updated 2019-06-12\n",
    "\n",
    "Resources:\n",
    "\n",
    "* [Official XGBoost python introduction](https://xgboost.readthedocs.io/en/latest/python/python_intro.html)\n",
    "* [XGBoost python demos](https://github.com/dmlc/xgboost/tree/master/demo/guide-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import scipy.sparse\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "Data for XGBoost models are stored in the [DMatrix](https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.DMatrix) data structure. Unlike the [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) structure from pandas, DMatrix appears optimized for computational speed rather than adhoc inspection of the data.\n",
    "\n",
    "XGBoost can [read data](https://xgboost.readthedocs.io/en/latest/python/python_intro.html#data-interface) from many formats, including csv files, pandas DataFrames, and more.\n",
    "\n",
    "One possible data workflow for an XGBoost project:\n",
    "1. Query the data from source tables.\n",
    "2. Inspect a sample of the raw data using pandas, matplotlib, and seaborn.\n",
    "3. Clean the raw data (using pandas for small data, or some more scalable solution for big data).\n",
    "4. Prepare the raw data (using natural language processing, [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) for categorical data, etc.).\n",
    "5. Train your boosting model.\n",
    "6. Evaluate performance, troubleshoot, understand, and visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from text files\n",
    "dtrain = xgb.DMatrix(PATH_DATA + 'agaricus.txt.train')\n",
    "dtest = xgb.DMatrix(PATH_DATA + 'agaricus.txt.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain.num_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain.num_row()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dtrain.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters via map\n",
    "param2 = {\n",
    "    'max_depth': 2,\n",
    "    'eta': 1,\n",
    "    'silent': 1,\n",
    "    'objective': 'binary:logistic'\n",
    "}\n",
    "\n",
    "param3 = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 1,\n",
    "    'silent': 1,\n",
    "    'objective': 'binary:logistic'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify validations to watch performance\n",
    "watchlist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "num_round = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "bst2 = xgb.train(param2,\n",
    "                 dtrain,\n",
    "                 num_round,\n",
    "                 watchlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bst3 = xgb.train(param3,\n",
    "                 dtrain,\n",
    "                 num_round,\n",
    "                 watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project to obtain predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bst3.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dtest.get_label()\n",
    "print('error=%f' % (sum(1 for i in range(len(preds)) if int(preds[i] > 0.5) != labels[i]) / float(len(preds))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "Per this [Medium article](https://medium.com/@srnghn/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3) by Stacey Ronaghan, **feature importance** is the decrease in node impurity, for nodes that split at that feature, weighted by the probability of reaching that node. The higher the value, the more important the feature.\n",
    "\n",
    "We can define **node impurity** as:\n",
    "$$\\text{Gini impurity} \\equiv \\sum_{i=1}^{C} -f_i(1 - f_i),$$\n",
    "where $f_i$ is the frequency of label $i$ at a node. Intuitively, impurity is greatest (in absolute value) when the variance in outcomes is high, and lowest (zero) when there is no variance in outcomes.\n",
    "\n",
    "If a split cleanly divides the outcomes into the possible label categories, then it has contributed greatly to the reduction in variance of the labels.\n",
    "\n",
    "If a large proportion of the samples pass through a split, then it has a high contribution to the overall variance decrease achieved by the complete tree.\n",
    "\n",
    "And, finally, if a feature is used repeatedly for splits that greatly decrease node impurity for a large proportion of samples, then that feature is important for the overall predictive power of the tree.\n",
    "\n",
    "**TODO:** What does feature importance look like if we have **highy correlated (multicollinear) features**? In regression: multicollinearity results in (1) unreliable point estimates, and (2) blown-up standard errors. It seems like having the same feature in there twice would kind of mask the importance of either feature... in this situation it would be useful to do a simple linear regresion alongside the tree-based model to get insight into these relationships within the data.\n",
    "\n",
    "**TODO:** See [Interpretable Machine Learning with XGBoost](https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27), a Medium article by Scott Lundberg, for a discussion of different ways to compute and visualize feature importance for tree-based models. His github repo for [SHAP](https://github.com/slundberg/shap), meaning SHapley Additive exPlanations, contains practical examples for his unified approach to explain the output of any machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(bst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(bst3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to normalize feature importances is to simply divide by the sum of all feature importances... but this might not actually be that informative because importances will then naturally be lower for models with more features. Ultimately we care about the relative feature importances and have to understand that the scale does not matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: not sure why plot_tree not working...\n",
    "# xgb.plot_tree(bst, num_trees=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "327px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
